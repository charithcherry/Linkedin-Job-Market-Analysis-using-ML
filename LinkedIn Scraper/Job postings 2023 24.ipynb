{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f44a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abf3e8",
   "metadata": {},
   "source": [
    "# Linkedin Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb308df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"job postings 2023 24/postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.original_listed_time[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_ddmmyyyy(unix_time):\n",
    "    # Convert from milliseconds to seconds\n",
    "    unix_time_seconds = unix_time / 1000.0\n",
    "    \n",
    "    # Convert to datetime and handle errors\n",
    "    formatted_dates = pd.to_datetime(unix_time_seconds, unit='s', errors='coerce')\n",
    "    \n",
    "    # Format as 'dd-mm-yyyy'\n",
    "    return formatted_dates.dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] = convert_unix_to_ddmmyyyy(raw_data['original_listed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231085eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] =pd.to_datetime(raw_data['original_listed_time_mod'], format='%d-%m-%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61667a47",
   "metadata": {},
   "source": [
    "# Data Scraped from ScrapingDog API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f820b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_all_linkedin_jobs(api_key, fields, geoids, pages):\n",
    "    url = \"https://api.scrapingdog.com/linkedinjobs/\"\n",
    "    all_data = []  # Store data from all requests\n",
    "\n",
    "    # Iterate over each combination of field, geoid, and page\n",
    "    for field in fields:\n",
    "        for geoid in geoids:\n",
    "            for page in pages:\n",
    "                params = {\n",
    "                    \"api_key\": api_key,\n",
    "                    \"field\": field,\n",
    "                    \"geoid\": geoid,\n",
    "                    \"page\": page\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Append data from each request to the list\n",
    "                    all_data.extend(response.json())\n",
    "                else:\n",
    "                    print(f\"Request failed for field: {field}, geoid: {geoid}, page: {page} with status code: {response.status_code}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Usage example\n",
    "api_key = \"671da7467fddaf7ca053001a\"\n",
    "fields = [\"java\", \"data science\", \"machine learning\"]\n",
    "geoids = [\"100293800\",\"103736294\",\"102277331\"]  # NV, Colorado, California\n",
    "pages = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "# data = fetch_all_linkedin_jobs(api_key, fields, geoids, pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    new_Data = pd.DataFrame(data)\n",
    "    new_Data.to_csv(\"261024_Job_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10045ce",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Filtering Data\n",
    "\n",
    "Filtering Function Overview\n",
    "The filter_data function dynamically filters a DataFrame based on a specified list of columns and corresponding conditions. This flexibility allows for customized data extraction based on various criteria, such as dates, maximum or minimum values, and frequent items.\n",
    "\n",
    "Function Parameters\n",
    "df (pd.DataFrame): The input DataFrame containing the data you want to filter.\n",
    "\n",
    "columns (list): A list of column names to apply filters on. Each column corresponds to a condition in the conditions list.\n",
    "\n",
    "conditions (list): A list of conditions, where each element is a tuple specifying the operation and the value for filtering. Each tuple matches one of the columns in the columns list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab432a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_data(df, columns, conditions):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on a list of columns and their conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to filter.\n",
    "    - columns (list): List of column names to apply filters on.\n",
    "    - conditions (list): List of conditions where each element is a tuple in the format:\n",
    "                         (condition, value), for example ('>', '2024-04-20').\n",
    "                         Each tuple should correspond to a column in the columns list.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Loop over each column and its corresponding condition\n",
    "    for column, (condition, value) in zip(columns, conditions):\n",
    "        if condition == '>':\n",
    "            df = df[df[column] > value]\n",
    "        elif condition == '<':\n",
    "            df = df[df[column] < value]\n",
    "        elif condition == '==':\n",
    "            df = df[df[column] == value]\n",
    "        elif condition == '>=':\n",
    "            df = df[df[column] >= value]\n",
    "        elif condition == '<=':\n",
    "            df = df[df[column] <= value]\n",
    "        elif condition == '!=':\n",
    "            df = df[df[column] != value]\n",
    "        elif condition == 'max':\n",
    "            max_value = df[column].max()\n",
    "            df = df[df[column] == max_value]\n",
    "        elif condition == 'min':\n",
    "            min_value = df[column].min()\n",
    "            df = df[df[column] == min_value]\n",
    "        elif condition == 'top_n':\n",
    "            top_n_counts = df[column].value_counts().nlargest(value)\n",
    "            df = df[df[column].isin(top_n_counts.index)]\n",
    "        elif condition == 'top_n':\n",
    "            top_n_counts = df[column].value_counts()#.nlargest(value)\n",
    "            df = df[df[column].isin(top_n_counts.index)]\n",
    "        else:\n",
    "            print(f\"Invalid condition '{condition}' for column '{column}'\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a4400",
   "metadata": {},
   "source": [
    "## Filtering Scraped Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "scraped_data = pd.read_csv(\"C:/Users/DELL/Downloads/261024_Job_Data.csv\")\n",
    "\n",
    "# Convert date column to datetime if filtering by date\n",
    "scraped_data['job_posting_date'] = pd.to_datetime(scraped_data['job_posting_date'], errors='coerce')\n",
    "\n",
    "# Specify columns and conditions\n",
    "columns = ['job_posting_date']\n",
    "conditions = [('>', '2024-04-20')]\n",
    "\n",
    "# Apply the filter function\n",
    "filtered_scraped_data = filter_data(scraped_data, columns, conditions)\n",
    "\n",
    "# Save the filtered data\n",
    "filtered_scraped_data.to_csv(\"filtered_scraped_job_data.csv\", index=False)\n",
    "\n",
    "# Print the filtered data\n",
    "print(filtered_scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33554ef-65be-45e6-bc67-bfeec59a3e92",
   "metadata": {},
   "source": [
    "## Filtering Current Kaggle Data to check top N job positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d0974-f47c-4c8d-b5ad-29c3f2c24360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kaggledata = pd.read_csv(\"job postings 2023 24/postings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b4ef0-d4db-4951-a303-cf52c5848893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_n_values(df, column, n):\n",
    "    # Get the top n most frequent values in the specified column\n",
    "    top_n_values = df[column].value_counts()#.nlargest(n)\n",
    "    return top_n_values\n",
    "columns = ['title']\n",
    "conditions = [('top_n',50)]\n",
    "top_15_job_roles = filter_data(kaggledata, columns, conditions)\n",
    "\n",
    "print(top_15_job_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283f025-29ad-4f02-8269-34800a75afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_job_roles['title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a11c2f-a5cc-41d0-baa2-e47f2338d96e",
   "metadata": {},
   "source": [
    "## Data cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec2ae9-c6dc-4b44-b5bc-c60384258da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube = kaggledata.pivot_table(\n",
    "    values=[\n",
    "        'med_salary', 'max_salary', 'min_salary', 'views', 'applies', 'normalized_salary'\n",
    "    ],\n",
    "    index=[\n",
    "        'location', 'company_name', 'title', 'formatted_work_type', 'remote_allowed', 'formatted_experience_level'\n",
    "    ],\n",
    "    aggfunc={\n",
    "        'med_salary': 'mean',           # Average median salary\n",
    "        'max_salary': 'max',            # Maximum salary\n",
    "        'min_salary': 'min',            # Minimum salary\n",
    "        'views': 'sum',                 # Total views\n",
    "        'applies': 'sum',               # Total applications\n",
    "        'normalized_salary': 'mean'     # Average normalized salary\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Display the resulting data cube\n",
    "print(\"Data Cube:\")\n",
    "print(data_cube)\n",
    "\n",
    "# Optional: Save to CSV for further analysis\n",
    "data_cube.to_csv('job_data_cube.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e22f7a-8495-4804-9990-65eab6fc33fa",
   "metadata": {},
   "source": [
    "## Dynamic data cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0131eab-d2dd-4106-865d-7679d0721b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_data_cube(df, company_name=None, title=None, location=None, work_type=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Apply filters based on user input\n",
    "    if company_name:\n",
    "        filtered_df = filtered_df[filtered_df['company_name'] == company_name]\n",
    "    if title:\n",
    "        filtered_df = filtered_df[filtered_df['title'] == title]\n",
    "    if location:\n",
    "        filtered_df = filtered_df[filtered_df['location'] == location]\n",
    "    if work_type:\n",
    "        filtered_df = filtered_df[filtered_df['formatted_work_type'] == work_type]\n",
    "    \n",
    "    # Check if filtered DataFrame is not empty\n",
    "    if filtered_df.empty:\n",
    "        return \"No data found for the specified filters.\"\n",
    "    \n",
    "    # Aggregate metrics for the filtered data\n",
    "    result = {\n",
    "        'Total Job Listings': len(filtered_df),\n",
    "        'Max Salary': filtered_df['max_salary'].max(),\n",
    "        'Min Salary': filtered_df['min_salary'].min(),\n",
    "        'Average Median Salary': filtered_df['med_salary'].mean(),\n",
    "        'Total Views': filtered_df['views'].sum(),\n",
    "        'Total Applications': filtered_df['applies'].sum(),\n",
    "        'Average Normalized Salary': filtered_df['normalized_salary'].mean()\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74843968-e7ac-4385-9ffc-f25363af85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dynamic_data_cube(kaggledata, company_name=\"ServiceNow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d686e-c9e4-40b6-a530-20bbe1520eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dynamic_data_cube(kaggledata, title=\"Full Stack Java Developer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736d63c-9bca-43ff-94a9-35b595ad2057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
