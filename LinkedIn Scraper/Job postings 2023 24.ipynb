{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f44a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abf3e8",
   "metadata": {},
   "source": [
    "# Linkedin Data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb308df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"job postings 2023 24/postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_ddmmyyyy(unix_time):\n",
    "    # Convert from milliseconds to seconds\n",
    "    unix_time_seconds = unix_time / 1000.0\n",
    "    \n",
    "    # Convert to datetime and handle errors\n",
    "    formatted_dates = pd.to_datetime(unix_time_seconds, unit='s', errors='coerce')\n",
    "    \n",
    "    # Format as 'dd-mm-yyyy'\n",
    "    return formatted_dates.dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] = convert_unix_to_ddmmyyyy(raw_data['original_listed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231085eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] =pd.to_datetime(raw_data['original_listed_time_mod'], format='%d-%m-%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61667a47",
   "metadata": {},
   "source": [
    "# Data Scraped from ScrapingDog API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f820b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_all_linkedin_jobs(api_key, fields, geoids, pages):\n",
    "    url = \"https://api.scrapingdog.com/linkedinjobs/\"\n",
    "    all_data = []  # Store data from all requests\n",
    "\n",
    "    # Iterate over each combination of field, geoid, and page\n",
    "    for field in fields:\n",
    "        for geoid in geoids:\n",
    "            for page in pages:\n",
    "                params = {\n",
    "                    \"api_key\": api_key,\n",
    "                    \"field\": field,\n",
    "                    \"geoid\": geoid,\n",
    "                    \"page\": page\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Append data from each request to the list\n",
    "                    all_data.extend(response.json())\n",
    "                else:\n",
    "                    print(f\"Request failed for field: {field}, geoid: {geoid}, page: {page} with status code: {response.status_code}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Usage example\n",
    "api_key = \"671da7467fddaf7ca053001a\"\n",
    "fields = [\"java\", \"data science\", \"machine learning\"]\n",
    "geoids = [\"100293800\",\"103736294\",\"102277331\"]  # NV, Colorado, California\n",
    "pages = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "# data = fetch_all_linkedin_jobs(api_key, fields, geoids, pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    new_Data = pd.DataFrame(data)\n",
    "    new_Data.to_csv(\"261024_Job_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10045ce",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Filtering Data\n",
    "\n",
    "Filtering Function Overview\n",
    "The filter_data function dynamically filters a DataFrame based on a specified list of columns and corresponding conditions. This flexibility allows for customized data extraction based on various criteria, such as dates, maximum or minimum values, and frequent items.\n",
    "\n",
    "Function Parameters\n",
    "df (pd.DataFrame): The input DataFrame containing the data you want to filter.\n",
    "\n",
    "columns (list): A list of column names to apply filters on. Each column corresponds to a condition in the conditions list.\n",
    "\n",
    "conditions (list): A list of conditions, where each element is a tuple specifying the operation and the value for filtering. Each tuple matches one of the columns in the columns list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab432a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_data(df, columns, conditions):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on a list of columns and their conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to filter.\n",
    "    - columns (list): List of column names to apply filters on.\n",
    "    - conditions (list): List of conditions where each element is a tuple in the format:\n",
    "                         (condition, value), for example ('>', '2024-04-20').\n",
    "                         Each tuple should correspond to a column in the columns list.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Loop over each column and its corresponding condition\n",
    "    for column, (condition, value) in zip(columns, conditions):\n",
    "        if condition == '>':\n",
    "            df = df[df[column] > value]\n",
    "        elif condition == '<':\n",
    "            df = df[df[column] < value]\n",
    "        elif condition == '==':\n",
    "            df = df[df[column] == value]\n",
    "        elif condition == '>=':\n",
    "            df = df[df[column] >= value]\n",
    "        elif condition == '<=':\n",
    "            df = df[df[column] <= value]\n",
    "        elif condition == '!=':\n",
    "            df = df[df[column] != value]\n",
    "        elif operation == 'max':\n",
    "            max_value = df[column].max()\n",
    "            df = df[df[column] == max_value]\n",
    "        elif operation == 'min':\n",
    "            min_value = df[column].min()\n",
    "            df = df[df[column] == min_value]\n",
    "        elif operation == 'top_n':\n",
    "            top_n_counts = df[column].value_counts().nlargest(value)\n",
    "            df = df[df[column].isin(top_n_counts.index)]\n",
    "        else:\n",
    "            print(f\"Invalid condition '{condition}' for column '{column}'\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a4400",
   "metadata": {},
   "source": [
    "## Filtering Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "scraped_data = pd.read_csv(\"C:/Users/DELL/Downloads/261024_Job_Data.csv\")\n",
    "\n",
    "# Convert date column to datetime if filtering by date\n",
    "scraped_data['job_posting_date'] = pd.to_datetime(scraped_data['job_posting_date'], errors='coerce')\n",
    "\n",
    "# Specify columns and conditions\n",
    "columns = ['job_posting_date']\n",
    "conditions = [('>', '2024-04-20')]\n",
    "\n",
    "# Apply the filter function\n",
    "filtered_scraped_data = filter_data(scraped_data, columns, conditions)\n",
    "\n",
    "# Save the filtered data\n",
    "filtered_scraped_data.to_csv(\"filtered_scraped_job_data.csv\", index=False)\n",
    "\n",
    "# Print the filtered data\n",
    "print(filtered_scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4823ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
