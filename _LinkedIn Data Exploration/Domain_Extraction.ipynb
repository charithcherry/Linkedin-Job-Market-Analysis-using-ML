{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32851f47-36de-48f1-ae75-afcb0ee90988",
   "metadata": {},
   "source": [
    "### Domain Clustering using K-means\n",
    "\n",
    "This Python code performs text preprocessing and clustering on job posting data. The steps involved are:\n",
    "\n",
    "1. **Text Preprocessing:**\n",
    "   - It loads a dataset of job postings (`postings.csv`), extracts the `title` and `description` columns, and processes them using the `nltk` library. \n",
    "   - The text is cleaned by removing non-word characters, extra spaces, converting to lowercase, and lemmatizing words while removing stopwords.\n",
    "\n",
    "2. **TF-IDF Vectorization:**\n",
    "   - A `TfidfVectorizer` is applied to the combined text (job title + description) to convert the text into numerical features for clustering.\n",
    "\n",
    "3. **Dimensionality Reduction:**\n",
    "   - Principal Component Analysis (PCA) is applied to reduce the dimensionality of the data to 2 components for visualization.\n",
    "\n",
    "4. **K-Means Clustering:**\n",
    "   - The code uses the Elbow Method to determine the optimal number of clusters by plotting the Within-Cluster Sum of Squares (WCSS).\n",
    "   - It performs K-Means clustering with 10 clusters and assigns cluster labels to the job postings.\n",
    "\n",
    "5. **Visualization:**\n",
    "   - The clusters are visualized using a scatter plot with the two principal components and marked cluster centers.\n",
    "\n",
    "6. **Result:**\n",
    "   - The resulting clusters are stored in the DataFrame, which is then displayed for the first 20 job postings.\n",
    "\n",
    "### Final Note\n",
    "\n",
    "While the clustering approach in the initial code did not yield meaningful results due to the lack of sufficient domain knowledge, we successfully switched to a domain-specific clustering approach. This approach involved matching job postings against predefined domains and sub-domains using associated keywords, calculating probabilities for each match. Each job posting was then assigned primary and secondary clusters based on the highest probability matches. This method provided more relevant and actionable clustering results for categorizing the job postings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cfa78-e626-4efc-8b43-6b55b4ab58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import logging\n",
    "import concurrent.futures\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "# logging.basicConfig(filename='preprocess_debug.log', level=logging.DEBUG, format='%(asctime)s %(message)s')\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c759c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"job postings 2023 24/postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data = data[['title','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dab331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "#     text = text.lower()  # Convert to lowercase\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])  # Lemmatize and remove stop words\n",
    "#     return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['job_title'] = data['title'].apply(preprocess_text)\n",
    "# data['job_description'] = data['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86614475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    \n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words if word not in stop_words])\n",
    "    \n",
    "    # Log the processed text\n",
    "#     logging.debug(f\"Processed text: {lemmatized_text}\")\n",
    "    return lemmatized_text\n",
    "\n",
    "# # Function to apply preprocessing to a batch of texts\n",
    "# def preprocess_batch(batch):\n",
    "#     return [preprocess_text_nltk(text) for text in batch]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data['title'] = req_data['title'].astype(str).fillna('no data')\n",
    "req_data['description'] = req_data['description'].astype(str).fillna('no data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88642439",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data['job_title'] = req_data['title'].apply(preprocess_text_nltk)\n",
    "req_data['job_description'] = req_data['description'].apply(preprocess_text_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488176c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data['text'] = req_data['job_title'] + ' ' + req_data['job_description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_none_job_title = req_data['job_title'].isnull().any()\n",
    "print(f\"'job_title' column has None values: {has_none_job_title}\")\n",
    "\n",
    "# Check if the 'job_description' column has any None values\n",
    "has_none_job_description = req_data['job_description'].isnull().any()\n",
    "print(f\"'job_description' column has None values: {has_none_job_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data['text'] = req_data['text'].astype(str).fillna('no data')\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(req_data['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da102500",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "num_clusters = 20  # Define the number of clusters to check\n",
    "for i in range(1, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(X_reduced)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Ensure the x and y axes have the same dimensions\n",
    "plt.plot(range(1, num_clusters), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bbab4-2c4a-4530-8eb1-f35ba44c4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92beaaa3-3ced-43ca-b913-95ff34225c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5600c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(X_reduced)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "req_data['cluster'] = kmeans.labels_\n",
    "\n",
    "# Print cluster centers\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans.labels_, cmap='viridis', marker='o')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='x')\n",
    "plt.title('K-Means Clustering with k=3')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_data[['title','description','cluster']][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d97663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f51ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86ab9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43047b0e-9a93-4ffd-85d8-e5848df4c2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
