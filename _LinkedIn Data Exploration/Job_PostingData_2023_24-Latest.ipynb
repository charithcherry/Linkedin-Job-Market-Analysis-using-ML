{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f44a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abf3e8",
   "metadata": {},
   "source": [
    "# Linkedin Data from Kaggle\n",
    "\n",
    "## Data Preprocessing\n",
    "### Converting Unix Timestamps\n",
    "#### Overview\n",
    "The convert_unix_to_ddmmyyyy function transforms Unix timestamps (in milliseconds) into a human-readable date format (dd-mm-yyyy). This step ensures the timestamps are interpretable and consistent for analysis and visualization.\n",
    "\n",
    "### Function Parameters\n",
    "unix_time (pd.Series or list): A collection of Unix timestamps in milliseconds to be converted.\n",
    "### Key Steps\n",
    "Adjusting for Datetime Conversion: Converts Unix timestamps from milliseconds to seconds for compatibility with datetime libraries.\n",
    "Datetime Transformation: Uses pandas to handle conversion to datetime objects while managing invalid timestamps.\n",
    "Formatting: Outputs the dates in the dd-mm-yyyy format for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fb308df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"job postings 2023 24/postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.original_listed_time[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_ddmmyyyy(unix_time):\n",
    "    # Convert from milliseconds to seconds\n",
    "    unix_time_seconds = unix_time / 1000.0\n",
    "    \n",
    "    # Convert to datetime and handle errors\n",
    "    formatted_dates = pd.to_datetime(unix_time_seconds, unit='s', errors='coerce')\n",
    "    \n",
    "    # Format as 'dd-mm-yyyy'\n",
    "    return formatted_dates.dt.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] = convert_unix_to_ddmmyyyy(raw_data['original_listed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231085eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'] =pd.to_datetime(raw_data['original_listed_time_mod'], format='%d-%m-%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['original_listed_time_mod'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9170ad-999f-459b-a677-cece6dbc2c82",
   "metadata": {},
   "source": [
    "# Fetching LinkedIn Jobs using ScrapingDog API\n",
    "\n",
    "## Overview\n",
    "This script fetches job postings from LinkedIn using the ScrapingDog API. It retrieves data for specific fields (job categories), locations (geo IDs), and pages, combining the results into a unified dataset.\n",
    "\n",
    "\n",
    "## Function Definition: `fetch_all_linkedin_jobs`\n",
    "\n",
    "### Purpose\n",
    "The `fetch_all_linkedin_jobs` function retrieves LinkedIn job data for specified parameters (fields, geo IDs, and pages) using API calls and compiles it into a single dataset.\n",
    "\n",
    "### Parameters\n",
    "- **`api_key` (str)**: Your API key for authentication with ScrapingDog.\n",
    "- **`fields` (list of str)**: Job categories or search terms to filter jobs (e.g., \"data science\", \"machine learning\").\n",
    "- **`geoids` (list of str)**: Geographic region codes (e.g., NV, Colorado, California).\n",
    "- **`pages` (list of str)**: Page numbers to fetch paginated results.\n",
    "\n",
    "### Return Value\n",
    "- **List of JSON objects**: Combined data retrieved from all API calls.\n",
    "\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd  # Required to save data as CSV\n",
    "\n",
    "def fetch_all_linkedin_jobs(api_key, fields, geoids, pages):\n",
    "    \"\"\"\n",
    "    Fetches LinkedIn job postings using the ScrapingDog API.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key (str): API key for authentication.\n",
    "    - fields (list): List of job categories to fetch (e.g., [\"java\", \"data science\"]).\n",
    "    - geoids (list): List of geographic region IDs to fetch jobs from.\n",
    "    - pages (list): List of pages to iterate over for paginated results.\n",
    "\n",
    "    Returns:\n",
    "    - list: A combined list of JSON objects with job data.\n",
    "    \"\"\"\n",
    "    url = \"https://api.scrapingdog.com/linkedinjobs/\"\n",
    "    all_data = []  # Store data from all requests\n",
    "\n",
    "    # Iterate over each combination of field, geoid, and page\n",
    "    for field in fields:\n",
    "        for geoid in geoids:\n",
    "            for page in pages:\n",
    "                params = {\n",
    "                    \"api_key\": api_key,\n",
    "                    \"field\": field,\n",
    "                    \"geoid\": geoid,\n",
    "                    \"page\": page\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Append data from each request to the list\n",
    "                    all_data.extend(response.json())\n",
    "                else:\n",
    "                    print(f\"Request failed for field: {field}, geoid: {geoid}, page: {page} with status code: {response.status_code}\")\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10045ce",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Filtering Data\n",
    "\n",
    "**Filtering Function Overview**\n",
    "The filter_data function dynamically filters a DataFrame based on a specified list of columns and corresponding conditions. This flexibility allows for customized data extraction based on various criteria, such as dates, maximum or minimum values, and frequent items.\n",
    "\n",
    "**Function Parameters**\n",
    "df (pd.DataFrame): The input DataFrame containing the data you want to filter.\n",
    "\n",
    "columns (list): A list of column names to apply filters on. Each column corresponds to a condition in the conditions list.\n",
    "\n",
    "conditions (list): A list of conditions, where each element is a tuple specifying the operation and the value for filtering. Each tuple matches one of the columns in the columns list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab432a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_data(df, columns, conditions):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on a list of columns and their conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to filter.\n",
    "    - columns (list): List of column names to apply filters on.\n",
    "    - conditions (list): List of conditions where each element is a tuple in the format:\n",
    "                         (condition, value), for example ('>', '2024-04-20').\n",
    "                         Each tuple should correspond to a column in the columns list.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Loop over each column and its corresponding condition\n",
    "    for column, (condition, value) in zip(columns, conditions):\n",
    "        if condition == '>':\n",
    "            df = df[df[column] > value]\n",
    "        elif condition == '<':\n",
    "            df = df[df[column] < value]\n",
    "        elif condition == '==':\n",
    "            df = df[df[column] == value]\n",
    "        elif condition == '>=':\n",
    "            df = df[df[column] >= value]\n",
    "        elif condition == '<=':\n",
    "            df = df[df[column] <= value]\n",
    "        elif condition == '!=':\n",
    "            df = df[df[column] != value]\n",
    "        elif condition == 'max':\n",
    "            max_value = df[column].max()\n",
    "            df = df[df[column] == max_value]\n",
    "        elif condition == 'min':\n",
    "            min_value = df[column].min()\n",
    "            df = df[df[column] == min_value]\n",
    "        elif condition == 'top_n':\n",
    "            top_n_counts = df[column].value_counts().nlargest(value)\n",
    "            df = df[df[column].isin(top_n_counts.index)]\n",
    "        elif condition == 'top_n':\n",
    "            top_n_counts = df[column].value_counts()#.nlargest(value)\n",
    "            df = df[df[column].isin(top_n_counts.index)]\n",
    "        else:\n",
    "            print(f\"Invalid condition '{condition}' for column '{column}'\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a4400",
   "metadata": {},
   "source": [
    "## Filtering Scraped Data by Date\n",
    "\n",
    "### Overview\n",
    "This process filters job posting data based on specific date criteria, ensuring only relevant records are retained for further analysis.\n",
    "\n",
    "### Steps\n",
    "1. Load Data: Load the scraped job data into a pandas DataFrame.\n",
    "2. Prepare Date Column: Convert the job_posting_date column to datetime format, handling any invalid entries gracefully.\n",
    "3. Define Filters: Specify the column (job_posting_date) and condition (> a specific date) to filter postings after a chosen date.\n",
    "4. Apply Filter: Use the filter_data function to extract rows meeting the criteria.\n",
    "5. Save and Review: Save the filtered dataset to a CSV file and display the results for verification.\n",
    "\n",
    "This ensures the dataset contains only job postings after the specified date, optimizing it for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "scraped_data = pd.read_csv(\"C:/Users/DELL/Downloads/261024_Job_Data.csv\")\n",
    "\n",
    "# Convert date column to datetime if filtering by date\n",
    "scraped_data['job_posting_date'] = pd.to_datetime(scraped_data['job_posting_date'], errors='coerce')\n",
    "\n",
    "# Specify columns and conditions\n",
    "columns = ['job_posting_date']\n",
    "conditions = [('>', '2024-04-20')]\n",
    "\n",
    "# Apply the filter function\n",
    "filtered_scraped_data = filter_data(scraped_data, columns, conditions)\n",
    "\n",
    "# Save the filtered data\n",
    "filtered_scraped_data.to_csv(\"filtered_scraped_job_data.csv\", index=False)\n",
    "\n",
    "# Print the filtered data\n",
    "print(filtered_scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33554ef-65be-45e6-bc67-bfeec59a3e92",
   "metadata": {},
   "source": [
    "## Filtering Current Kaggle Data to check top N job positions\n",
    "\n",
    "This code processes a dataset of job postings to identify and extract the most common job titles. Here's a breakdown of its functionality:\n",
    "\n",
    "1. **Load Dataset**:\n",
    "The dataset postings.csv is loaded into a pandas DataFrame called kaggledata.\n",
    "\n",
    "2. **Top-N Filtering Function**:\n",
    "The filter_data function (previously defined) is used to filter the top 50 most frequent job titles (title column) based on their frequency in the dataset. The filtering uses the top_n condition to identify and retain rows corresponding to the most frequently occurring job titles.\n",
    "\n",
    "3. **Get Top Job Roles**:\n",
    "The filtered dataset is stored in the variable top_15_job_roles, which now contains rows corresponding to the top job titles.\n",
    "\n",
    "4. **Unique Job Titles**:\n",
    "The .unique() method is applied to the title column of top_15_job_roles to extract the unique job titles among the top 50, showing a list of distinct job roles.\n",
    "\n",
    "**Purpose**:\n",
    "The code identifies and highlights the most common job titles in the dataset, which can be useful for trend analysis, visualization, or reporting the popularity of specific roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d0974-f47c-4c8d-b5ad-29c3f2c24360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kaggledata = pd.read_csv(\"job postings 2023 24/postings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b4ef0-d4db-4951-a303-cf52c5848893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_n_values(df, column, n):\n",
    "    # Get the top n most frequent values in the specified column\n",
    "    top_n_values = df[column].value_counts()#.nlargest(n)\n",
    "    return top_n_values\n",
    "columns = ['title']\n",
    "conditions = [('top_n',50)]\n",
    "top_15_job_roles = filter_data(kaggledata, columns, conditions)\n",
    "\n",
    "print(top_15_job_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283f025-29ad-4f02-8269-34800a75afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_job_roles['title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a11c2f-a5cc-41d0-baa2-e47f2338d96e",
   "metadata": {},
   "source": [
    "## Creating a Data Cube for Job Analysis\n",
    "This code generates a data cube from a dataset of job postings, summarizing key metrics across multiple dimensions. Here's what it does:\n",
    "\n",
    "**Pivot Table Creation**:\n",
    "The pivot_table method is used to aggregate the dataset (kaggledata) based on specified values and dimensions.\n",
    "Metrics such as salaries, views, and applications are calculated using aggregation functions, grouped by dimensions like location, company name, job title, work type, remote flexibility, and experience level.\n",
    "\n",
    "**Metrics**:\n",
    "med_salary: Average median salary (mean).\n",
    "max_salary: Highest salary value (max).\n",
    "min_salary: Lowest salary value (min).\n",
    "views: Total job views (sum).\n",
    "applies: Total number of applications (sum).\n",
    "normalized_salary: Average normalized salary (mean).\n",
    "\n",
    "**Dimensions**:\n",
    "location: Geographic location of the job.\n",
    "company_name: Name of the hiring company.\n",
    "title: Job title or role.\n",
    "formatted_work_type: Type of employment (e.g., full-time, part-time).\n",
    "remote_allowed: Whether the job allows remote work.\n",
    "formatted_experience_level: Required experience level.\n",
    "\n",
    "**Reset Index**:\n",
    "The reset_index method is used to flatten the hierarchical index created by the pivot table, making it a regular DataFrame for easier manipulation and analysis.\n",
    "\n",
    "**Output the Data Cube**:\n",
    "The resulting data_cube is printed to display aggregated insights about the dataset.\n",
    "An optional CSV file (job_data_cube.csv) is saved for external analysis or reporting.\n",
    "\n",
    "**Purpose**:\n",
    "This code aggregates and organizes job posting data, enabling multi-dimensional analysis of salaries, views, and applications across factors like location, role, and experience level. The resulting data cube simplifies trend identification, comparisons, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec2ae9-c6dc-4b44-b5bc-c60384258da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cube = kaggledata.pivot_table(\n",
    "    values=[\n",
    "        'med_salary', 'max_salary', 'min_salary', 'views', 'applies', 'normalized_salary'\n",
    "    ],\n",
    "    index=[\n",
    "        'location', 'company_name', 'title', 'formatted_work_type', 'remote_allowed', 'formatted_experience_level'\n",
    "    ],\n",
    "    aggfunc={\n",
    "        'med_salary': 'mean',           # Average median salary\n",
    "        'max_salary': 'max',            # Maximum salary\n",
    "        'min_salary': 'min',            # Minimum salary\n",
    "        'views': 'sum',                 # Total views\n",
    "        'applies': 'sum',               # Total applications\n",
    "        'normalized_salary': 'mean'     # Average normalized salary\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Display the resulting data cube\n",
    "print(\"Data Cube:\")\n",
    "print(data_cube)\n",
    "\n",
    "# Optional: Save to CSV for further analysis\n",
    "data_cube.to_csv('job_data_cube.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e22f7a-8495-4804-9990-65eab6fc33fa",
   "metadata": {},
   "source": [
    "# Dynamic Data Cube with Filters\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `dynamic_data_cube` function is designed to extract and aggregate metrics from a DataFrame based on user-specified filters such as company name, job title, location, and work type.\n",
    "\n",
    "## Key Functionality\n",
    "\n",
    "### Filtering Data\n",
    "The function applies filters to the input DataFrame (`df`) based on the following optional parameters:\n",
    "- **`company_name`**: Filters rows with a specific company name.\n",
    "- **`title`**: Filters rows for a particular job title.\n",
    "- **`location`**: Filters rows by location.\n",
    "- **`work_type`**: Filters rows by work type (e.g., remote, on-site).\n",
    "\n",
    "### Validation\n",
    "If the filtered DataFrame is empty (i.e., no data matches the provided filters), the function returns a message indicating no results were found.\n",
    "\n",
    "### Aggregation\n",
    "For the filtered data, the function calculates the following metrics:\n",
    "- **`Total Job Listings`**: Count of job listings in the filtered dataset.\n",
    "- **`Max Salary`**: The highest salary from the filtered data.\n",
    "- **`Min Salary`**: The lowest salary from the filtered data.\n",
    "- **`Average Median Salary`**: The average of the median salaries in the filtered data.\n",
    "- **`Total Views`**: The total number of views across all filtered job listings.\n",
    "- **`Total Applications`**: The total number of applications received for the filtered job listings.\n",
    "- **`Average Normalized Salary`**: The average normalized salary in the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0131eab-d2dd-4106-865d-7679d0721b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_data_cube(df, company_name=None, title=None, location=None, work_type=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Apply filters based on user input\n",
    "    if company_name:\n",
    "        filtered_df = filtered_df[filtered_df['company_name'] == company_name]\n",
    "    if title:\n",
    "        filtered_df = filtered_df[filtered_df['title'] == title]\n",
    "    if location:\n",
    "        filtered_df = filtered_df[filtered_df['location'] == location]\n",
    "    if work_type:\n",
    "        filtered_df = filtered_df[filtered_df['formatted_work_type'] == work_type]\n",
    "    \n",
    "    # Check if filtered DataFrame is not empty\n",
    "    if filtered_df.empty:\n",
    "        return \"No data found for the specified filters.\"\n",
    "    \n",
    "    # Aggregate metrics for the filtered data\n",
    "    result = {\n",
    "        'Total Job Listings': len(filtered_df),\n",
    "        'Max Salary': filtered_df['max_salary'].max(),\n",
    "        'Min Salary': filtered_df['min_salary'].min(),\n",
    "        'Average Median Salary': filtered_df['med_salary'].mean(),\n",
    "        'Total Views': filtered_df['views'].sum(),\n",
    "        'Total Applications': filtered_df['applies'].sum(),\n",
    "        'Average Normalized Salary': filtered_df['normalized_salary'].mean()\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74843968-e7ac-4385-9ffc-f25363af85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dynamic_data_cube(kaggledata, company_name=\"ServiceNow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d686e-c9e4-40b6-a530-20bbe1520eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dynamic_data_cube(kaggledata, title=\"Full Stack Java Developer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b42813-8eab-4809-a2b2-bb1d28a37257",
   "metadata": {},
   "source": [
    "# Merging Job Data with Industry Information\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code demonstrates how to merge two datasets: one containing job postings and another containing industry information for companies. The datasets are merged based on the `company_id` column to provide a combined dataset with job information alongside the corresponding industry details.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Load the Datasets**\n",
    "   - The `jobs_df` dataset contains job postings.\n",
    "   - The `industries_df` dataset contains information about company industries.\n",
    "\n",
    "   Both datasets are loaded from CSV files using `pd.read_csv()`.\n",
    "\n",
    "2. **Merge the Datasets**\n",
    "   - The two datasets are merged on the `company_id` column using a **left join**. This ensures that all job postings are retained, and industry information is added wherever available.\n",
    "\n",
    "3. **Save the Merged Dataset**\n",
    "   - After merging, the combined dataset is saved to a new CSV file called `merged_jobs_with_industries.csv`.\n",
    "\n",
    "4. **Confirmation Message**\n",
    "   - A message is printed to indicate the successful completion of the merge and the addition of the industry column.\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "jobs_df = pd.read_csv(\"C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/postings.csv\") \n",
    "industries_df = pd.read_csv('C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/companies/company_industries.csv') \n",
    "\n",
    "# Merge the datasets on the company_id column\n",
    "merged_df = jobs_df.merge(industries_df, on='company_id', how='left')  # Perform a left join\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_df.to_csv('merged_jobs_with_industries.csv', index=False)\n",
    "\n",
    "print(\"Merge completed. The industries column has been added.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bce00-fc16-4b50-9220-7021b61172a8",
   "metadata": {},
   "source": [
    "# Dynamic Data Cube with Industries Included\n",
    "\n",
    "## Overview\n",
    "\n",
    "This function generates a dynamic data cube by filtering job postings based on various criteria, including the company name, job title, location, work type, and industry. After applying the filters, it aggregates key metrics such as salary, views, and applications.\n",
    "\n",
    "## Functionality\n",
    "\n",
    "1. **Filter the Data**  \n",
    "   The function allows users to filter the data based on multiple criteria:\n",
    "   - **Company Name**: Filters by the company offering the job.\n",
    "   - **Title**: Filters by job title.\n",
    "   - **Location**: Filters by job location.\n",
    "   - **Work Type**: Filters by work type (e.g., full-time, part-time).\n",
    "   - **Industry**: Filters by the job's industry, with a case-insensitive search.\n",
    "\n",
    "2. **Aggregation of Key Metrics**  \n",
    "   After applying the filters, the following metrics are calculated:\n",
    "   - Total job listings\n",
    "   - Maximum salary\n",
    "   - Minimum salary\n",
    "   - Average median salary\n",
    "   - Total views\n",
    "   - Total applications\n",
    "   - Average normalized salary\n",
    "\n",
    "3. **Return Results**  \n",
    "   If no data matches the filter criteria, a message is returned stating that no data was found. Otherwise, the function returns the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1c38cf-aa28-4d86-8007-ec5a95bf2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_data_cube(df, company_name=None, title=None, location=None, work_type=None, industry=None):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Apply filters based on user input\n",
    "    if company_name:\n",
    "        filtered_df = filtered_df[filtered_df['company_name'] == company_name]\n",
    "    if title:\n",
    "        filtered_df = filtered_df[filtered_df['title'] == title]\n",
    "    if location:\n",
    "        filtered_df = filtered_df[filtered_df['location'] == location]\n",
    "    if work_type:\n",
    "        filtered_df = filtered_df[filtered_df['formatted_work_type'] == work_type]\n",
    "    if industry:\n",
    "        filtered_df = filtered_df[filtered_df['industry'].str.contains(industry, case=False, na=False)]  # Case-insensitive\n",
    "    # Check if filtered DataFrame is not empty\n",
    "    if filtered_df.empty:\n",
    "        return \"No data found for the specified filters.\"\n",
    "    \n",
    "    # Aggregate metrics for the filtered data\n",
    "    result = {\n",
    "        'Total Job Listings': len(filtered_df),\n",
    "        'Max Salary': filtered_df['max_salary'].max(),\n",
    "        'Min Salary': filtered_df['min_salary'].min(),\n",
    "        'Average Median Salary': filtered_df['med_salary'].mean(),\n",
    "        'Total Views': filtered_df['views'].sum(),\n",
    "        'Total Applications': filtered_df['applies'].sum(),\n",
    "        'Average Normalized Salary': filtered_df['normalized_salary'].mean()\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84917fc7-f969-4ad6-8114-34f96bf5141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Job Listings': 343, 'Max Salary': np.float64(307900.0), 'Min Salary': np.float64(15.5), 'Average Median Salary': nan, 'Total Views': np.float64(4887.0), 'Total Applications': np.float64(293.0), 'Average Normalized Salary': np.float64(145290.2548387097)}\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data_cube(merged_df, company_name= \"Amazon\", industry=\"Software Development\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c578adcb-ba4d-44b7-ac39-2990e044a93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Job Listings': 1304, 'Max Salary': np.float64(330000.0), 'Min Salary': np.float64(15.0), 'Average Median Salary': np.float64(17364.422), 'Total Views': np.float64(10317.0), 'Total Applications': np.float64(688.0), 'Average Normalized Salary': np.float64(88354.88512024048)}\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data_cube(merged_df, industry= \"Banking\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d253491-b8b5-4a2a-a433-a316de33705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Job Listings': 16553, 'Max Salary': np.float64(950000.0), 'Min Salary': np.float64(10.0), 'Average Median Salary': np.float64(26060.236622418877), 'Total Views': np.float64(114444.0), 'Total Applications': np.float64(7766.0), 'Average Normalized Salary': np.float64(360982.0397253541)}\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_data_cube(merged_df, industry= \"Health care\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e50c6-9a4f-4a94-b20a-7fb042f8ca8b",
   "metadata": {},
   "source": [
    "# Industry Insights Metrics\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code calculates industry-specific metrics by grouping job postings based on the `industry` column and then aggregating key job-related statistics for each industry. The aggregated metrics include the average maximum salary, average minimum salary, total views, and total applications.\n",
    "\n",
    "## Functionality\n",
    "\n",
    "1. **Group by Industry**  \n",
    "   The dataset (`merged_df`) is grouped by the `industry` column, so that metrics can be calculated for each industry.\n",
    "\n",
    "2. **Aggregate Metrics**  \n",
    "   For each industry, the following metrics are calculated:\n",
    "   - **Average Maximum Salary** (`max_salary`): The mean of all maximum salaries for job postings in the industry.\n",
    "   - **Average Minimum Salary** (`min_salary`): The mean of all minimum salaries for job postings in the industry.\n",
    "   - **Total Views** (`views`): The sum of all views for job postings in the industry.\n",
    "   - **Total Applications** (`applies`): The sum of all applications for job postings in the industry.\n",
    "\n",
    "3. **Store and Display Results**  \n",
    "   The results are stored in a dictionary under the key `Industry Insights`, where each industry and its associated metrics are stored as a list of dictionaries. The `result` dictionary is then printed, showing the industry-specific insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b47b50-d3ad-46f6-86ff-546d91d03945",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_metrics = merged_df.groupby('industry').agg({\n",
    "    'max_salary': 'mean',\n",
    "    'min_salary': 'mean',\n",
    "    'views': 'sum',\n",
    "    'applies': 'sum'\n",
    "}).reset_index()\n",
    "result = {}\n",
    "result['Industry Insights'] = industry_metrics.to_dict('records')\n",
    "print(result['Industry Insights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb69a44-68c2-4e02-a178-bc08c05d7510",
   "metadata": {},
   "source": [
    "# Visualizing Salary Data by Industry\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code creates a bar plot using Plotly to visualize the maximum and minimum salaries across different industries. The plot displays the industry names on the x-axis and the corresponding maximum and minimum salaries on the y-axis.\n",
    "\n",
    "## Functionality\n",
    "\n",
    "1. **Plot Creation with Plotly Express**  \n",
    "   The `px.bar()` function is used to create a bar plot. The plot displays:\n",
    "   - **x-axis**: The `industry` column, which represents different industries.\n",
    "   - **y-axis**: Two columns, `max_salary` and `min_salary`, representing the maximum and minimum salary for each industry.\n",
    "\n",
    "2. **Labels and Title**  \n",
    "   - The title of the plot is set to `\"Max and Min Salary by Industry\"`.\n",
    "   - The y-axis labels are customized to display \"Max Salary\" for `max_salary` and \"Min Salary\" for `min_salary`.\n",
    "\n",
    "3. **Rendering the Plot**  \n",
    "   The `plot(fig)` function is used to display the plot in a Jupyter Notebook or other compatible environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfcf4170-3ce9-4b7e-94f8-20d21879b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.offline import plot\n",
    "# Create a bar plot with Plotly\n",
    "fig = px.bar(industry_metrics, x='industry', y=['max_salary', 'min_salary'], \n",
    "             title=\"Max and Min Salary by Industry\", labels={'max_salary': 'Max Salary', 'min_salary': 'Min Salary'})\n",
    "\n",
    "# To ensure Plotly works in Jupyter, you might need to explicitly render it\n",
    "plot(fig)\n",
    "pio.write_html(fig, file='salary_data_by_industry.html', auto_open=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
