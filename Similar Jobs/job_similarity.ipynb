{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c623c6b-fe6f-4af7-8241-9b3f17899c56",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing, Model Training, and Storing the Model as a Pickle File\n",
    "\n",
    "### Data Preprocessing: \n",
    "Preprocess the text columns (title, description, skills_desc) in the dataset.\n",
    "### TF-IDF Vectorization: \n",
    "Perform TF-IDF vectorization on the preprocessed text data.\n",
    "### Cosine Similarity Calculation: \n",
    "Store the cosine similarity matrix as a h5py file, so it can be used later for making recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f303f8-42f5-4188-b8c4-203578d17075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f5afc-1cb1-4667-bfb1-7db2a97bee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e859c65-2f52-4454-b5b1-10f020bb19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable for TF-IDF Vectorizer and Cosine Similarity Matrix\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10000)  # Limit the number of features\n",
    "cosine_sim_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be663c-f75d-4c71-9a26-fbeee233ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936adcc8-15f8-409a-a416-8c3d5c940162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and lowercase the text\n",
    "    text = re.sub(r'\\W', ' ', text.lower())\n",
    "    # Tokenize and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658172e-39fb-43e2-b79d-83e203c649f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_store_model(data_path=\"C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/postings.csv\"):\n",
    "#     # Load data\n",
    "#     data = pd.read_csv(data_path)\n",
    "#     new_directory = \"D:/ARJYAHI/Models\"\n",
    "#     os.makedirs(new_directory, exist_ok=True)\n",
    "    \n",
    "#     # Combine the relevant text columns into a single string for each job\n",
    "#     data[\"combined_text\"] = data[\"title\"].fillna('') + ' ' + data[\"description\"].fillna('') + ' ' + data[\"skills_desc\"].fillna('')\n",
    "#     data[\"combined_text\"] = data[\"combined_text\"].apply(preprocess_text)\n",
    "    \n",
    "#     # Perform TF-IDF vectorization\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "    \n",
    "#     # Calculate cosine similarity between all jobs and keep it sparse\n",
    "#     global cosine_sim_matrix\n",
    "#     cosine_sim_matrix = cosine_similarity(tfidf_matrix, dense_output=False)  # Keep it sparse\n",
    "    \n",
    "#     # Store the TF-IDF vectorizer and the sparse cosine similarity matrix in an HDF5 file\n",
    "#     h5_file_path = os.path.join(new_directory, 'model_data.h5')\n",
    "#     with h5py.File(h5_file_path, 'w') as h5f:\n",
    "#         # Save the TF-IDF matrix (sparse matrix) as a dense dataset\n",
    "#         h5f.create_dataset('tfidf_matrix', data=tfidf_matrix.toarray(), compression='gzip')\n",
    "        \n",
    "#         # Save the cosine similarity matrix as a dense dataset\n",
    "#         h5f.create_dataset('cosine_sim_matrix', data=cosine_sim_matrix, compression='gzip')\n",
    "        \n",
    "#         # Save the pickled TF-IDF vectorizer in the HDF5 file as a serialized object\n",
    "#         tfidf_vectorizer_pickle = pickle.dumps(tfidf_vectorizer)\n",
    "#         h5f.create_dataset('tfidf_vectorizer', data=np.void(tfidf_vectorizer_pickle))\n",
    "\n",
    "#     print(\"Model and cosine similarity matrix have been stored successfully in HDF5 format.\")\n",
    "\n",
    "def process_and_store_model(data_path= 'C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/postings.csv' ):\n",
    "    # Load job postings data\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # Combine the relevant text columns into a single string for each job\n",
    "    data[\"combined_text\"] = data[\"title\"].fillna('') + ' ' + data[\"description\"].fillna('') + ' ' + data[\"skills_desc\"].fillna('')\n",
    "    data[\"combined_text\"] = data[\"combined_text\"].apply(preprocess_text)\n",
    "    \n",
    "    # Perform TF-IDF vectorization\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "    \n",
    "    # Use Approximate Nearest Neighbors with cosine similarity metric\n",
    "    nbrs = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute').fit(tfidf_matrix)\n",
    "\n",
    "    # Save the model and TF-IDF vectorizer to an HDF5 file\n",
    "    model_directory = \"D:/ARJYAHI/Models\"\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "    h5_file_path = os.path.join(model_directory, 'model_data.h5')\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'w') as h5f:\n",
    "        # Serialize the TF-IDF vectorizer with pickle and store in HDF5\n",
    "        tfidf_vectorizer_pickle = pickle.dumps(tfidf_vectorizer)\n",
    "        h5f.create_dataset('tfidf_vectorizer', data=np.void(tfidf_vectorizer_pickle))\n",
    "        # Save the NearestNeighbors model directly (or save parameters if large)\n",
    "        nbrs_pickle = pickle.dumps(nbrs)\n",
    "        h5f.create_dataset('nbrs', data=np.void(nbrs_pickle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a9928-c333-4166-8478-e425ea62bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_store_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488c446-5e0b-4c6a-bb67-fd27b2cb4576",
   "metadata": {},
   "source": [
    "## Part B: Loading the Files and Using the Model for Recommendations\n",
    "In Part B, we load the files and use them to make recommendations based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f844370-94f4-492f-af70-adf3194cca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import h5py\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e07d14-7307-4529-9b9c-461c1d27f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedeefc-c569-4672-a6b8-894b20df062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess and lemmatize text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and lowercase the text\n",
    "    text = re.sub(r'\\W', ' ', text.lower())\n",
    "    # Tokenize and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b4f87-80f4-4139-8d2c-c614d5e79322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model_and_recommend(title=None, skills=None, top_n=10):\n",
    "#     # Define the model directory and load the HDF5 file\n",
    "#     model_directory = \"D:/ARJYAHI/Models\"\n",
    "#     h5_file_path = os.path.join(model_directory, 'model_data.h5')\n",
    "    \n",
    "#     # Open the HDF5 file and load the TF-IDF vectorizer and cosine similarity matrix\n",
    "#     with h5py.File(h5_file_path, 'r') as h5f:\n",
    "#         # Load the TF-IDF vectorizer from the serialized pickle data\n",
    "#         tfidf_vectorizer_pickle = h5f['tfidf_vectorizer'][()]\n",
    "#         tfidf_vectorizer = pickle.loads(tfidf_vectorizer_pickle.tobytes())\n",
    "        \n",
    "#         # Load the cosine similarity matrix\n",
    "#         cosine_sim_matrix = h5f['cosine_sim_matrix'][:]\n",
    "\n",
    "#     # Ensure at least one input is provided\n",
    "#     if not title and not skills:\n",
    "#         raise ValueError(\"Please provide at least a title or skills to get recommendations.\")\n",
    "    \n",
    "#     # Combine title and skills, if provided, into one input text\n",
    "#     input_text = ''\n",
    "#     if title:\n",
    "#         input_text += title\n",
    "#     if skills:\n",
    "#         input_text += ' ' + skills\n",
    "#     input_text = preprocess_text(input_text)  # Only preprocess the user input here\n",
    "    \n",
    "#     # Transform the input text using the loaded TF-IDF vectorizer\n",
    "#     input_tfidf = tfidf_vectorizer.transform([input_text])\n",
    "    \n",
    "#     # Calculate cosine similarity between the input and all job postings\n",
    "#     cosine_sim = cosine_similarity(input_tfidf, cosine_sim_matrix)\n",
    "    \n",
    "#     # Load the job data (same dataset as during training)\n",
    "#     data = pd.read_csv(\"C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/postings.csv\")\n",
    "    \n",
    "#     # Get similarity scores for the input\n",
    "#     sim_scores = list(enumerate(cosine_sim[0]))\n",
    "#     # Sort by similarity score in descending order\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # Get the indices of the top_n most similar jobs\n",
    "#     sim_indices = [i[0] for i in sim_scores[:top_n]]\n",
    "    \n",
    "#     # Return the top_n similar jobs\n",
    "#     return data.iloc[sim_indices][[\"job_id\", \"company_name\", \"title\", \"description\", \"skills_desc\", \"location\"]]\n",
    "\n",
    "def load_model_and_recommend(title=None, skills=None, top_n=10):\n",
    "    model_directory = \"D:/ARJYAHI/Models\"\n",
    "    h5_file_path = os.path.join(model_directory, 'model_data.h5')\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'r') as h5f:\n",
    "        # Load the TF-IDF vectorizer and NearestNeighbors model from HDF5\n",
    "        tfidf_vectorizer = pickle.loads(h5f['tfidf_vectorizer'][()].tobytes())\n",
    "        nbrs = pickle.loads(h5f['nbrs'][()].tobytes())\n",
    "    \n",
    "    # Combine title and skills into input text and preprocess\n",
    "    input_text = ' '.join(filter(None, [title, skills]))\n",
    "    input_text = preprocess_text(input_text)\n",
    "    \n",
    "    # Transform the input text using the loaded TF-IDF vectorizer\n",
    "    input_tfidf = tfidf_vectorizer.transform([input_text])\n",
    "    \n",
    "    # Find nearest neighbors (most similar jobs)\n",
    "    distances, indices = nbrs.kneighbors(input_tfidf, n_neighbors=top_n)\n",
    "    \n",
    "    # Load the job postings dataset\n",
    "    data = pd.read_csv(\"C:/Users/DELL/Linkedin-Job-Market-Analysis-using-ML/LinkedIn Scraper/job postings 2023 24/postings.csv\")\n",
    "    \n",
    "    # Return the top_n similar jobs\n",
    "    return data.iloc[indices[0]][[\"job_id\", \"company_name\", \"title\", \"description\", \"skills_desc\", \"location\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2482fe-533c-4998-9e9c-dfc6025ea2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_input = \"Data Scientist\"  # Or set it to None if only using skills\n",
    "skills_input = None  # Or set it to a skills string if only using skills\n",
    "similar_jobs = load_model_and_recommend(title=title_input, skills=skills_input)\n",
    "print(similar_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd865d5-ae53-400d-930b-2f8c12b8bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
